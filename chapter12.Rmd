--- 
title       : "Regressions 1: Introduction to Regression Analysis"
description : "This chapter will introduce you to regression analysis"
 
 
  
--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1
## Introduction to Regression Analysis
*** =video_link
//player.vimeo.com/video/216023860
 
 
 
 --- type:MultipleChoiceExercise lang:r xp:50 skills:1 
## Interpreting Regressions
The popular online auctioneer, eGulf, wants to provide its sellers recommended sales prices for their used WePhones, based on how old the WePhones are. They examine a small sample of 25 WePhones sold in the past week, and run an OLS regression to determine the relationship between WePhone age and sales price to factor into their algorithm. They then plot their results (illustrated in the R workspace). Each point on the plot indicates the age and price that a particular phone sold at, whereas the line shows the results from an OLS regression on this data. What do these results suggest about the relationship of WePhone age and sales prices?

*** =instructions
- There is a negative relationship
- There is no relationship
- There is a positive relationship
*** =pre_exercise_code
```{r}
set.seed(1)
library(ggplot2)
WePhone<-data.frame(age=rep(c(1,2,3,4,5),5))
WePhone$Feedback<-round(rnorm(n=25,mean=90,sd=3))
WePhone$Age<-WePhone$age+rnorm(n=25,mean=0,sd=.3)-WePhone$Feedback/10+mean(WePhone$Feedback/10)
WePhone$Value<-500-(9*WePhone$age^2)+ round(rnorm(n=25,mean=0,sd=30))
names(WePhone)[4]<-"Price Sold"
WePhone$`Price Sold`<-WePhone$`Price Sold`+20*(WePhone$Feedback-mean(WePhone$Feedback))
model<-lm(`Price Sold`~Age,data=WePhone)
ggplot(data=WePhone,aes(Age, `Price Sold`))+geom_point()+geom_abline(intercept = model$coefficients[1],slope=model$coefficients[2])+ ggtitle("Scatter Plot and OLS Regression of WePhone Age on Price Sold")
```
*** =sct
```{r}
msg1 = "Correct! As the age of WePhones increased, the price at which they were sold decreased."
msg2 = "Although the plot does not indicate if they results were statistically significant, they do indicate a relationship. Try again"
msg3 = "Think this through carefully. Does an increase in age increase or decrease the price sold of a WePhone? Try again"
test_mc(correct = 1, feedback_msgs = c(msg1,msg2,msg3))
```



--- type:MultipleChoiceExercise lang:r xp:50 skills:1 
## Reversing Causal Direction
Causal interpretation of regression models can be tricky. Using the same data in the last example, eGulf decided to run a regression model where age is a function of price sold (i.e. age is the dependent variable in this model). The results of this model are visualized in the R workspace. If this model shows a causal relationship between age and price sold, with age as the dependent variable, what **exactly** would that mean?

*** =instructions
- Older phones tend to sell for less
- Old age causes phones to sell for less
- High sales prices causes phones to become younger
*** =pre_exercise_code
```{r}
set.seed(1)
library(ggplot2)
WePhone<-data.frame(age=rep(c(1,2,3,4,5),5))
WePhone$Feedback<-round(rnorm(n=25,mean=90,sd=3))
WePhone$Age<-WePhone$age+rnorm(n=25,mean=0,sd=.3)-WePhone$Feedback/10+mean(WePhone$Feedback/10)
WePhone$Value<-500-(9*WePhone$age^2)+ round(rnorm(n=25,mean=0,sd=30))
names(WePhone)[4]<-"Price Sold"
WePhone$`Price Sold`<-WePhone$`Price Sold`+20*(WePhone$Feedback-mean(WePhone$Feedback))
model<-lm(Age~`Price Sold`,data=WePhone)
ggplot(data=WePhone,aes(`Price Sold`,Age))+geom_point()+geom_abline(intercept = model$coefficients[1],slope=model$coefficients[2])+ ggtitle("Scatter Plot and OLS Regression of WePhone Age on Price Sold")
```
*** =sct
```{r}
msg1 = "This is not a causal interpretation of the relationship, but rather a description of the association between WePhone age and sales price"
msg2 = "This is the most intuitive explanation for the relationship between WePhone sale price and age, but not what a regression model where age is the dependent variable implies. Try again"
msg3 = "Correct! This is an example of reverse causality. The age of any object can only be influenced by time (excluding time dilation), so we know that price sold cannot cause a phone's age."
test_mc(correct = 3, feedback_msgs = c(msg1,msg2,msg3))
```



--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1
## Basic Elements of a Regression Table
*** =video_link
//player.vimeo.com/video/216023860



--- type:MultipleChoiceExercise lang:r xp:50 skills:1 
## Reading a Bivariate Regression Table
The following table shows the regression results from our previous question that tested the effect age on the price sold of used WePhones. The coefficient for age matches the slope in the previous line graph. It suggests that for each additional year (or unit of age), the expected sales price of WePhones decreases by $61.99. The coefficient for intercept matches the Y-axis intercept in the previous line graph. It indicates the price for a WePhone that was 0 years old (more generally, it shows the value of the outcome when all other independent variables are at their reference point). To determine the predicted sales price of a WePhone with a given age, one can simply add the coefficient for the intercept with the coefficient for age times the number of year old the WePhone is. What is the predicted price for a WePhone that is exactly two years old ?

| Variable   | Coefficient (SE) |
|------------|-----------------:|
|    Age     |   -61.99 (7.28)  |
| Intercept  |   592.56 (24.42) |


*** =instructions
- 406.49
- 468.58
- 530.57 
- 592.56
*** =sct
```{r}
msg1 = "Try again"
msg2 = "Well done! Notice any advantages of regression models versus t.tests? In a t.test, we can only predict the outcome from a binary treatment variable (that is, the outcome for respondents in the treatment versus control group), In a regression model, we can predict the outcome from a continuous treatment variable (e.g. age)."
msg3 = "Try again"
msg4 = "Try again"
test_mc(correct = 3, feedback_msgs = c(msg1,msg2,msg3,msg4))
```



--- type:MultipleChoiceExercise lang:r xp:50 skills:1 
## Reading a Multivariate (Multiple) Regression Table
In our previous eGulf example, we assumed that age is the only determinant of sales price. Sellers on eGulf also have "feedback scores," ranging from 0-100, that indicate the quality of sellers based on their interactions with previous customers. What if a seller's feedback score had a big impact on a WePhone's sales price, and was negatively correlated with the age of WePhones (i.e. highly rates sellers tend to sell newer WePhones)? This would confound the relationship between sales price and age. 

In a t.test, we would probably want to balance our WePhone samples by seller feedback score, possibly by throwing out respondents who are not matched between samples. However, with OLS regression, we can simply add seller feedback scores as another determinant of our independent variable, thereby controlling for its confounding effect on our outcome variable. This is what is shown in the table below.

When controlling for seller score, age still has a large negative effect on sales. Using this table, calculate the difference in expected sales price for phone that is 1 year old with a feedback score of 90, versus a phone that is 4 years old with a feedback score of 90.

| Variable   | Coefficient (SE) |
|------------|-----------------:|
|    Age     |  -55.61 (4.94)   |
|  Feedback  |   14.15 (2.55)   |
| Intercept  | -707.23 (234.38) |


*** =instructions
- 162.85
- 166.83 
- 177.00
- 191.15
*** =sct
```{r}
msg1 = "Try again"
msg2 = "Well done! The same method can applied with as many variables as we choose. However, notice that the intercept is -707.23. That would indicate that a used WePhone with an age of 0 and a feedback score of 0 would have a predicted sales price of -$707.23. Obviously that is incorrect. This hints at a modeling issue that we'll explore later in the chapter."
msg3 = "Try again"
msg4 = "Try again"
test_mc(correct = 2, feedback_msgs = c(msg1,msg2,msg3,msg4))
```




--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1
## The Relationship between Economic Development and Property Rights
*** =video_link
//player.vimeo.com/video/216023860




--- type:MultipleChoiceExercise lang:r xp:50 skills:1 
## Multiple Regression Models
Papers often report several regression models next to each other. This table shows a few truncated models from the example in the video, about the relationship between a country's property rights protections and its GDP. Model 1 shows the coefficient and standard error for the effect of property rights protection on GDP with no controls. Model 2 shows the same effect controlling for the country's latitude, and Model 3 shows the effect controlling for whether the country is in Asia. Why might the effect of property rights protections on GDP decrease as we add more variables to our model?   

| Variable   | Model 1 | Model 2 | Model 3 |
|------------|--------:|--------:|---------|
| Protection |.52(.06) |.47(.06) |.43(.05) |
| Latitude   |         |.89(.49) |.37(.51) |
| Asia Dummy |         |         |-.62(.19)|


*** =instructions
- Because the inclusion of more variables weakens the statistical power of an independent variable
- Because ommitting these variables caused the effect of property rights protection on GDP to be downwardly biased. 
- Because the inclusion of more variables strengthens the statistical power of an independent variable
- Because ommitting these variables caused the effect of property rights protection on GDP to be upwardly biased. 
*** =sct
```{r}
msg1 = "Although this is often true, this is not indicated in this figure. The statistical significance of property rights protections on GDP is relatively consistent across models. Try again."
msg2 = "This would mean that the omitted variables confound the relationship between property rights protections and GDP by decreasing the it. Try again"
msg3 = "This is usually not true, nor shown in this figure. Try again."
msg4 = "Correct! Without controlling for these variables, the relationship between property rights protections and GDP seemed larger than it truly was."
test_mc(correct = 4, feedback_msgs = c(msg1,msg2,msg3,msg4))
```



--- type:NormalExercise lang:r aspect_ratio:62.5 xp:50 skills:1
## Practice Running a Regression Model
Dr. Max Funn is interested in understanding the effect of income on happiness. He has a small sample of the General Social Survey (GSS), and plans to use this information to measure the effect of income on happyiness. Although Funn predicts that income increases happiness, he is worried that the relationship between income and happiness might be confounded by their joint relationship with work hours. Specifically, Funn believes that people who work more hours make more money, but that working more hours tends to make people less happy. 

Using the dataset, `GSS`, help Dr. Max Funn measure the effect of income on happiness, **controlling** for work hours. Specifically:


*** =instructions
- Construct a regression model that measures the effect of income on happiness.
- Construct a second regression model that measures the effect of income on happiness, controlling for work hours.

*** =pre_exercise_code
```{r}
n=1000
set.seed(1)
#Create rnorm function that allows for min and max
  rtnorm <- function(n, mean, sd, min = -Inf, max = Inf){
      qnorm(runif(n, pnorm(min, mean, sd), pnorm(max, mean, sd)), mean, sd)
  }
#Create rounding function that allows to round to numbers above 1
  mround <- function(x,base){ 
          base*round(x/base) 
  } 
#Create scaling function that puts number between 0 and 1
  scale <- function(x){
    (x - min(x))/(max(x)-min(x))
    }

#Dataframe
  GSS<-data.frame(WorkHours=1:n,Income=1:n,Happiness=1:n)
#Work hours 
  GSS$WorkHours<-round(rtnorm(n=n,mean=40,sd=6,min=0,max=55))
#Income (in thousands)
  GSS$Income<-GSS$WorkHours*.5+mround(rtnorm(n=n,mean=30,sd=5,min=0,max=100),.1)
#Scaled variables
  scaledincome<-scale(GSS$Income)
  scaledWorkHours<-scale(GSS$WorkHours)
#Happiness (set as 1:5; rounded to 1; make income 2* as effective as work hours)
  GSS$Happiness<-round(scaledincome-(.5*scaledWorkHours) + rtnorm(n=n,mean=3,sd=.5,min=0,max=5),0)
  
  summary(GSS$WorkHours)
  summary(GSS$Income)
  summary(GSS$Happiness)
    cor(GSS$WorkHours,GSS$Income)
    
```
*** =sample_code
```{r}
# Before running a regression model, let's examine the data
  str(GSS)
# There are three variables: income (measured in thousands of dollars), work hours, and happiness (measured 1=very unhappy, 2=unhappy, 3=niether happy or unhappy, 4=happy, 5=very happy)
  summary(GSS$Income)
  summary(GSS$WorkHours)
  summary(GSS$Happiness)
# Using the correlation function, we can tell that income and happiness are positively correlated
  cor(GSS$Income,GSS$Happiness)
# But by how much? What is the effect of increasing one's income by $1000 on their happiness?
  
# Use the glm function to regress the effect of income on happiness. For reference, the syntax for glm is "glm(Y~X,data=df)" where Y is the name of the independant variable, X is the name of the dependent variable, and df is the dataframe.
#---- Question 1-------------------------------------#
      Solution1<-glm()
#----------------------------------------------------#

# The glm function usually gives less data than desired, and usually in a less than ideal format. Let's use the summary function on our model to see its effects.
  summary(Solution1)
  Solution1$coefficients
# The table beneath "Coefficients" is what is usually reported in scientific journals. "Estimate" refers to the coefficients for each paramater, "Pr(>|t|)" refers to the p.value of the estames, and the stars following p.value refer to the result's statistical significance. If you entered Solution1 correctly, the effect of Income should be about .015, meaning that a unit ($1000) increase in income is associated with a .015 increase in happiness. The low p.value and stars next to it suggest that this effect is statistically significant. This relationship can also be viewed graphically via the syntax below:
  plot(GSS$Happiness ~ GSS$Income)
  abline(Solution1)
# The points on the plot represent each observation, and the diagonal line shows the regression results. Note that the slope of the line is equal to the coefficient for income.
# Now let's move on to Question 2. The following correlations show that work hours is negatively correlated with happiness and positively correlated with income. This suggests that work hours might confound the relationship between income and happiness.
  cor(GSS$WorkHours,GSS$Happiness)
  cor(GSS$WorkHours,GSS$Income)
#We therefore want to control for the effect of work hours on happiness. Use the glm function to regress the effect of income on happiness. For reference, the syntax for glm with controls is "glm(Y~X+Z,data=df)" where Y is the name of the independant variable, X is the name of the dependent variable, Z is the name of the control variable, and df is the dataframe.
#---- Question 2-------------------------------------#
      Solution2<-glm()
#----------------------------------------------------#
  
```
*** =solution
```{r}
    Solution1<-glm(Happiness~Income,data=GSS)
    Solution2<-glm(Happiness~Income+WorkHours,data=GSS)
```
*** =sct
```{r}
test_object("Solution1")
test_object("Solution2")
success_msg("Good work! You may have noticed that the coefficient for income increased when we controlled for WorkHours. This is typical when a control variable is positively correlated with the independent variable and positively correlated with the dependent variable. It is also possible to use this syntax to control for as many variables as desired in a regression model.")
```



--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1
## Using Regression to Get Causal Effects: Unconfoundedness
*** =video_link
//player.vimeo.com/video/216023860



--- type:NormalExercise lang:r aspect_ratio:62.5 xp:50 skills:1
## Practice Running a Regression Model With Interaction Effects
An experiment conducted by the transportation network company, Unter Technologies, tried to determine whether downsizing their Human Resources (HR) department would lead to higher employee turnover. Using t.tests, they found that the effect of downsizing their HR department would have different conditional average treatment effects (CATE) on men and women. This was determined by separately studying the average treatment effect (ATE) of downsizing on a sample of men and on a sample of just women. However, slicing data this way can substantially reduce one's statistical power, and becomes unwieldy when a data scientist wants to determine more than a couple of CATEs with a given sample. For example, if race is also and important factor in whether employees plan to leave Unter if Unter reduces their HR department, we would need to slice the data several more times, and run several distinct t.tests on the data.

A CATE is basically an example of statistical moderation (also known as an interaction effect), where the effect of an independent variable is moderated by the effect of a second independent variable. In this example, we would say that gender moderates the effect of the treatment (downsizing HR) on intention to leave Unter Technologies. With the dataframe, `UnterHR`, construct three regression models: One that naively estimates the average treatment effect of reducing the size of Unter's HR department on employee turnover, a second that includes an interaction between treatment and gender (`Female`), and a third that includes interactions between treatment and gender (`Female`) and treatment and race (`Race`).

*** =instructions
- Construct a regression model that measures the effect of `treatment` on `LeaveJob`
- Construct a regression model that measures the effect of `treatment` on `LeaveJob`, with an interaction with `Female`
- Construct a regression model that measures the effect of `treatment` on `LeaveJob`, with separate interactions with `Female` and `Race`
*** =pre_exercise_code
```{r}
set.seed(1)
n=682
#Create Dataframe
  UnterHR<-data.frame(Treatment=rbinom(n,1,.4),Female=rbinom(n,1,.1),LeaveJob=0,Race=sample(4,n,prob=c(.6,.1,.1,.2),replace=T))
#Rename race
  UnterHR$Race<-as.factor(ifelse(UnterHR$Race==1,"White",ifelse(UnterHR$Race==2,"Black",ifelse(UnterHR$Race==3,"Latino","Asian"))))
#LeaveJob
  #treatment makes men less likely to leave
    UnterHR$LeaveJob[UnterHR$Treatment==0 & UnterHR$Female==0]<-rbinom(length(UnterHR$LeaveJob[UnterHR$Treatment==0 & UnterHR$Female==0]),1,.3)
    UnterHR$LeaveJob[UnterHR$Treatment==1 & UnterHR$Female==0]<-rbinom(length(UnterHR$LeaveJob[UnterHR$Treatment==1 & UnterHR$Female==0]),1,.2)
  #treatment makes wommen more likely to leave
    UnterHR$LeaveJob[UnterHR$Treatment==0 & UnterHR$Female==1]<-rbinom(length(UnterHR$LeaveJob[UnterHR$Treatment==0 & UnterHR$Female==1]),1,.3)
    UnterHR$LeaveJob[UnterHR$Treatment==1 & UnterHR$Female==1]<-rbinom(length(UnterHR$LeaveJob[UnterHR$Treatment==1 & UnterHR$Female==1]),1,.6)
  #treatment makes blacks and latinos more likely to leave (redraw if leave job = 0 and and black or latino = 1)
    #count people in either category
      n2<-length(UnterHR$LeaveJob[UnterHR$Treatment==1 & UnterHR$LeaveJob==0 & (UnterHR$Race=="Black" | UnterHR$Race=="Latino" )])
    #redraw for those
      UnterHR$LeaveJob[UnterHR$Treatment==1 & UnterHR$LeaveJob==0 & (UnterHR$Race=="Black" | UnterHR$Race=="Latino" )]<-rbinom(n2,1,.5)
      
      
```
*** =sample_code
```{r}

```
*** =solution
```{r}
      summary(glm(LeaveJob ~ Treatment,data=UnterHR))
      summary(glm(LeaveJob ~ Treatment * Female,data=UnterHR))
      summary(glm(LeaveJob ~ Treatment * Female + Treatment *Race,data=UnterHR))
```
*** =sct
```{r}
test_object("Solution1")
test_object("Solution2")
success_msg("Good work!")
```